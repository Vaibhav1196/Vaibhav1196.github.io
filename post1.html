<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Backpropogation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">



		
		.TextWrapLeft
        {
			clear: left.
			float: left;
			margin: 10px;
		}



		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Return to main page</a>
					</header>


				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">February 16, 2020</span>
									<h1>The 4 Horsemen of Backpropogation</h1>
									
								</header>
								<div>
									<img style="margin: 5px 20px 0 0;" ALIGN="left" src="images/post1/The-four-horsemen.png"  width="400" height="400"  style="border:5px solid black"/>
								</div> 

								<p  style="color:black;font-size:18px;"> 
									
									The structure of the equations shown in the figure (for example: using the Hadamard product instead of Matrix multiplication)
									is inspired from the book "Neural Networks and Deeplearning" by "Michael Nielsen". 
									This book is highly recommended if one is looking for a mathematically intensive approach to the topic. In this article we will not dive into the 
									backpropogation algorithm itself but try and understand the relevence of these equations and how certain terms appear in them. This post can be used as a 
									supplement while understanding backpropogation or like a cheatsheet if one has understood the mathematical view point of this algorithm.
									Before going into the discussion of each equation we will quickly establish what each term in the set of four equations mean considering a pictorial representation of DNN. 
								</p>



								<div>
									<img style="margin: 20px 5px 0 0;" ALIGN="right"  src="images/post1/DNN.png"  width="500" height="375"  style="border:5px solid black"/>
								</div> 
								
								<p  style="color:black;font-size:18px;x"> 
									
									The figure on the right having the configuration of 3-4-2 DNN structure depicts the various terms in the network. <b>"w" is the weight arriving to the corresponding neuron.
									The weight "w" is moving to the "j<sup>th</sup>" neuron in the "l<sup>th</sup>" layer from the "k<sup>th</sup>" neuron in the "(l-1)<sup>th</sup>" layer The weighted sum at a neuron in layer "l" with bias "b" is represented by "z".
									 A point to remember here is that the bias "b" is local to a neuron while the weights "w" are not. The activation function is represented by "&sigma;" and the activation function of a neuron is given by "&sigma;(z)".
									Here the output of the activation is represented by "a" , that is "&sigma;(z) = a".</b>     
								</p>
								<p  style="color:black;font-size:18px;"> 
									
									The <b>Cost function C</b> considered here is given by the quadratic function as shown in the figure. <b>Here "a<sub>j</sub><sup>L</sup>" is the final prediction at the L<sup>th</sup> layer and the j<sup>th</sup> neuron.
									 and y<sub>j</sub> is the ground truth at the j<sup>th</sup> output neuron of the input data point. Here an important point to note is that the ground truth is constant thus the cost C is purely a function of "a<sup>L</sup>" , Cost C = C(a<sup>L</sup>)</b>. 
								</p>
								
                                <p  style="color:black;font-size:18px;"> 
							   
									Now we will consider the most important term that we are interested in "<b>&delta;". Here consider the neuron marked in light pink , the blue arrows indicate the incoming weights "w" to the neuron while the red arrows indicate the outgoing activation terms "a".
								 When the weighted sum "z" with the incoming blue weights "w" along with the bias "b" in the neuron is computed it induces an error &delta;<sub>j</sub><sup>l</sup> ; at the j<sup>th</sup> neuron and the l<sup>th</sup> layer. 
								 Futher this error propogates through the rest of the network as it effects the activation &sigma; as shown in the figure and effects the output "a<sup>L</sup>". Since se know that the Cost "C" is completely dependent on "a<sup>L</sup>" in the final layer 
								, the term that would be interesting for us is the partial derivative "&part;C/&part;z<sub>j</sub><sup>l</sup>" which tells us the rate of change of the Cost function with respect to the weighted sum "z" which includes the incoming weights "w" and bias "b" of the neuron. </b>
							 
								</p>

								
							</section>

					</div>

			

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>