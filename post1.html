<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Backpropogation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	

  
		<head>
        <style>
			.floatleft
		{
		float: left;
		width: 100px;
		}

		.center 
		{
        display: block;
        margin-left: auto;
        margin-right: auto;
        width: 50%;
        }

		h1.uppercase 
		{
        text-transform: uppercase;
        }
        h1.lowercase 
		{
        text-transform: lowercase;
        }
        h1.capitalize 
		{
        text-transform: capitalize;
        }
		h1.both
		{
		text-transform: none;
		}
		p.newspace 
		{
		line-height: 2;
        margin-top: 0;
        margin-bottom: 0;
        text-indent: 22px;
        } 
		
		
		</style>
		</head>

		<body class="is-preload"></body>
		
		
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Return to main page</a>
					</header>


				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">February 16, 2020</span>
									<h1>The 4 Horsemen of Backpropogation</h1>

									<p>This post is about the 4 equations that make backpropogation great in deep neural networks. 
										This is a topic providing a different view point of the equations and answering questions involving 
										on how and why certain terms exist within the equations. Having a strong mathematical background in backpropogation is recomededded 
										in order to appreciate this post.</p>
									
								</header>


								<div>
									
									<img style="margin: 5px 20px 0 0;" ALIGN="left" src="images/post1/The-four-Horsemen.png"  width="400" height="400"  style="border:5px solid black"/>
									
									
								</div> 

								<p  style="color:black;font-size:18px;"> 
									
									The structure of the equations shown in figure 1 (for example: using the Hadamard product instead of Matrix multiplication)
									is inspired from the book "Neural Networks and Deeplearning" by "Michael Nielsen". 
									This book is highly recommended if one is looking for a mathematically intensive approach to the topic. In this article we will not dive into the 
									backpropogation algorithm itself but try and understand the relevence of these equations and how certain terms appear in them. This post can be used as a 
									supplement while understanding backpropogation or like a cheatsheet if one has understood the mathematical view point of this algorithm.
									Before discussing about each equation we will quickly establish what each term in the set of four equations mean; considering a pictorial representation of a Deep Neural Network (DNN). 
								</p>



								<div>
									
									<img style="margin: 0 0 5px 20px;" ALIGN="right"  src="images/post1/DNN.png"  width="500" height="375"  style="border:5px solid black"/>
								
								</div> 
								

								<p  style="color:black;font-size:18px;"> 
									
									Figure 2 on the right with the configuration of 3-4-2 DNN structure depicts the various terms in the network. <b>'w'</b> is the weight arriving to the corresponding neuron.
									The weight 'w' is moving to the <b>'j<sup>th</sup>'</b> neuron in the <b>'l<sup>th</sup>'</b> layer from the <b>'k<sup>th</sup>'</b> neuron in the <b>'(l-1)<sup>th</sup>'</b> layer. The weighted sum at a neuron in layer <b>'l'</b> with bias <b>'b'</b> is represented by <b>'z'</b>.
									 A point to remember here is that the bias <b>'b'</b> is local to a neuron while the weights <b>'w'</b> are not. The activation function is represented by <b>'&sigma;'</b> and the activation function of a neuron is given by <b>'&sigma;(z)'</b>.
									Here the output of the activation is represented by <b>'a'</b> , that is <b>&sigma;(z) = a</b>.   
								</p>
								<p  style="color:black;font-size:18px;"> 
									
									The <b>Cost function C</b> considered here is given by the quadratic equation as shown in the figure. Here <b>'a<sub>j</sub><sup>L</sup>'</b> is the final prediction at the <b>'L<sup>th</sup>'</b> layer and the <b>'j<sup>th</sup>'</b> neuron.
									 and <b>'y<sub>j</sub>'</b> is the ground truth at the <b>'j<sup>th</sup>'</b> output neuron of the input data point. Here an important point to note is that the ground truth is constant thus the cost C is purely a function of <b>'a<sup>L</sup>'</b> , <b>Cost C = C(a<sup>L</sup>)</b>. 
								</p>
								
                                <p  style="color:black;font-size:18px;"> 
							   
									Now we will consider the most important term that we are interested in '<b>&delta;'</b>. Here consider the neuron marked in light pink , the blue arrows indicate the incoming weights <b>'w'</b> to the neuron while the red arrows indicate the outgoing activation terms <b>'a'</b>.
								 When the weighted sum <b>'z'</b> with the incoming blue weights <b>'w'</b> along with the bias <b>'b'</b> in the neuron is computed it induces an error <b>'&delta;<sub>j</sub><sup>l</sup>'</b>' ; at the <b>'j<sup>th</sup></b>' neuron and the <b>'l<sup>th</sup>'</b> layer. 
								 Futher this error propogates through the rest of the network as it effects the activation <b>'&sigma;'</b> as shown in the figure and eventually effects the output <b>'a<sup>L</sup>'</b>. Since we know that the <b>Cost 'C'</b> is completely dependent on <b>'a<sup>L</sup>'</b> in the final layer 
								, the term that would be interesting for us is the partial derivative <b>&part;C/&part;z<sub>j</sub><sup>l</sup></b> which tells us the rate of change of the Cost function with respect to the weighted sum <b>'z'</b> which includes the incoming weights <b>'w'</b> and bias <b>'b'</b> of the neuron.
								Thus if we can somehow get to know the partial derivative <b>&part;C/&part;w<sub>jk</sub><sup>l</sup></b> at the <b>'j<sup>th</sup>'</b> neuron in the <b>'l<sup>th</sup>'</b> layer , this will give 
								us information on how the cost function is changing with respect to a wight <b>'w'</b> in some random neuron in the network. Finally this can be used to modify the weights in our network and thus allowing us to modify the <b>'&delta;<sub>j</sub><sup>l</sup>'</b> value with a motto of reducing the Cost function.
								This was a fast recap of the essentials before we start discussing about each equations and this sets the stage for futher discussions. The gif below shows the pictorial recap of the working of backpropogation.
								Since we have set the stage for our article now we can finally start appreciating the math and have a natuarl understanding of the four equations in figure 1.
								</p>

								<div>
									
									<img  class="center" ALIGN="bottom" alt="Backpropogation gif example"  src="images/post1/How-Weights-Change.gif" width="800" height="400"  style="border:5px solid black">
							        <p style="text-align: center;font-size:15px;">Figure 3: Backpropogation working montage</p>
								    
								</div> 




								     <!--************************************************START OF EQUATION ONE***************************************************************-->

								
									<h1 class="lowercase" style="color:black;font-size:40px;" > 1) &delta;<sup>L</sup> = &nabla;<sub>a</sub>C &odot; &sigma;'(z<sup>L</sup>) </h2>
									
									
									 <p style="color:black;font-size:18px;"> Now that we have set the platform to discuss about the four equations we can start with the first one which is  <b>&delta;<sup>L</sup> = &nabla;<sub>a</sub>C &odot; &sigma;'(z<sup>L</sup>)</b>.
										but before discussing this matrix format of the equation we will review the same equation but with a touch of calculus. In calculus format the above equation is given as 
										<b>&delta;<sup>L</sup><sub>j</sub> = &part;C/&part;a<sup>L</sup><sub>j</sub> &sigma;'(z<sup>L</sup><sub>j<sub></b>). But how did we arrive at this expression ?
										I always prefer the pen and paper approach to enlighten myself during such doubts. We will consider the pictorial representation of a DNN with the required terms as shown in figure 4 along with the pen and paper derivation.
									</p> 
							

								<div>
									
									<img style="margin: 5px 20px 0 0;" ALIGN="left" src="images/post1/the-first-horsemen.png"  width="350" height="370"  style="border:5px solid black"/>
										
								</div> 	
   

								<p  style="color:black;font-size:18px;"> 
									
									Figure 4 on the left shows a pictorial representation of a DNN with <b>N</b> layers. Now we will consider the neuron marked in light pink in layer <b>l</b>. The <b>'z<sub>j</sub><sup>l</sup>'</b> is the weighted sum of the incoming weights to the neuron
									along with the bias in that neuron. Once we apply the activation <b>&sigma;(z<sub>j</sub><sup>l</sup>)</b> we get the output for that neuron as <b>'a<sub>j</sub><sup>l</sup>'</b>. Here l represents the layer and j the j<sup>th</sup> neuron in the corresponding layer.
									We had previously established that the error <b>&delta;<sup>l</sup>j<sub><sub></b> induced inside the neuron can be approximated to the rate of change of the final output cost function at the prediction layer with respect top the weighted sum in the neuron of interest. 
									This is given by the following partial derivative expression <b>&delta;<sup>l</sup>j<sub></sub>=&part;C/&part;z<sub>j</sub><sup>l</sup></b>.
									Thus by doing this we are trying to see how an error in one neuron is affecting the final prediction as it propogates through the other layers in the forward direction , hence the partial derivative. 
									Now using these infomation along with a pen and paper we will see why we end up getting <b>&delta;<sup>L</sup><sub>j</sub> = &part;C/&part;a<sup>L</sup><sub>j</sub> &sigma;'(z<sup>L</sup><sub>j<sub></b>).
								  
								</p>

								<div>
									
									<img  class="center" ALIGN="bottom" alt="How we arrive at first equation"  src="images/post1/why-first-equation.jpg" width="800" height="700"  style="border:5px solid black">
							        <p style="text-align: center;font-size:15px;">Figure 5: How we arrive at first equation</p>
								    
								</div> 

								<p  style="color:black;font-size:18px;"> 
									
									As shown in the old school method of pen and paper in the figure 5 we see how we arrive at the corresponding equation. Thus the error <b>&delta;<sup>l</sup>j<sub></sub></b> in the j<sup>th</sup> 
									nueron of the l<sup>th</sup> is given by the product of: 
								  
								</p>

								<p  class="newspace"style="color:black;font-size:18px;"> 

									<b>1)   How fast the the cost 'C' is changing as a function of the activated output 'a'. If the Cost 'C' does not depend on a particular
										output of a neuron 'j' then its '&delta;<sup>l</sup>j<sub></sub>' will be small.
									</b>
								</p>

								<p  style="color:black;font-size:18px;"> 
									
									<b>2)   How fast the activation function '&sigma;' is changing with respect to its input weighted sum 'z<sup>l</sup><sub>j</sub>'.
									</b>
								</p>

								
								<p  style="color:black;font-size:18px;"> 
									
									From the above two points we have now understood how the error evolves as it propogates through the network and also this way of writing the equation gives a 
									natural flow in understanding how the various terms in the equation effect the final cost function that we are interested in to minimize. Another important thing 
									to consider here is that the product between the two terms in the equation is an elementwise operation , thus it is the <b>Hadamard product</b> that is used while 
									representing the above equation in matrix form.
								</p>

								<p  style="color:black;font-size:18px;"> 
									
									As we have gained enough insight using calculus and why certain terms appear in the equation , we can now write the equation 
								    <b>&delta;<sub>j</sub><sup>l</sup></b> in matrix form as: 
								
									<h1 class="lowercase" style="color:black;font-size:30px;text-align:center;"> &delta;<sup>L</sup> = &nabla;<sub>a</sub>C &odot; &sigma;'(z<sup>L</sup>) </h2>

								</p>

								<p  style="color:black;font-size:18px;"> 
									
									Now this equation considers one layer in the DNN and represnts all the errors of that layer in a vector form , hence represented as <b>&delta;<sup>l</sup>;</b> where <b>'l'</b> represents the layer.
									The &nabla; operator also called the nabla operator is used to represent the partial derivative of <b>'C'</b> with respect to <b>'a'</b> here. <b>&nabla;<sub>a</sub>C</b> is the rate of change of C with respect 
									to output activations represented as a vector. Finally we have the Hadamard product or the elementwise product <b>&odot;</b> with <b>&sigma;'(z<sup>l</sup>)</b>. 
									Now considering the cost function as a quadratic function as shown previously ; we can rewrite the expression <b>&nabla;<sub>a</sub>C</b> = <b>(a<sup>L</sup>-y)</b>. 
									We get this term when we take the partial derivative of the Cost <b>C</b> with respect to <b>a</b> (<b>&part;C/&part;a</b>). Thus we can rewrite the equation for <b>&delta;<sub>j</sub><sup>l</sup></b> as:

									<h1 class="lowercase" style="color:black;font-size:30px;text-align:center;"> &delta;<sup>L</sup> = (a<sup>L</sup>-y) &odot; &sigma;'(z<sup>L</sup>) </h2>

								</p>


								<p  style="color:black;font-size:18px;"> 
									
									An important thing to remember is that , this equation is used to get the error <b>&delta;<sup>L</sup></b> in the forward direction. That is this is the feed forward part 
									of our algorithm where we see how the error is propogated to the end layer eventually affecting the output prediction. In the next equation we will check out the backpropogation of the error.
								</p>


                                <!--************************************************END OF EQUATION ONE***************************************************************-->


							    <!--************************************************START OF EQUATION TWO***************************************************************-->
							
							
								<h1 class="both" style="color:black;font-size:40px;" > 2) &delta;<sup>L</sup> = ((w<sup>l+1</sup>)<sup>T</sup> &delta;<sup>l+1</sup>) &odot; &sigma;'(z<sup>L</sup>) </h2>


								
							</section>


							
						

					</div>

			

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>